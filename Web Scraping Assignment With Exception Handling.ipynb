{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aead5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessory Libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException , StaleElementReferenceException, ElementNotInteractableException \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dab2b0",
   "metadata": {},
   "source": [
    "### 1.Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‚Äòguitar‚Äô. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b01585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Name Of product-- fruits\n",
      "Exception Raised--\n"
     ]
    }
   ],
   "source": [
    "#Define Function\n",
    "def amazon_search(product):\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get('https://www.amazon.in/')\n",
    "        time.sleep(2)\n",
    "        #Xpath\n",
    "        search_bar = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "        search_bar.send_keys(product)\n",
    "        time.sleep(2)\n",
    "        search_button = driver.find_element(By.XPATH,'//span[@id=\"nav-search-submit-text\"]') #Xpath\n",
    "        search_button.click()\n",
    "        time.sleep(10)\n",
    "    except NoSuchElementException:\n",
    "        print('Exception Raised--')\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter Name Of product-- \")  #User input\n",
    "    amazon_search(user_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20e99e",
   "metadata": {},
   "source": [
    "### 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and ‚ÄúProduct URL‚Äù. In case, if any of the details are missing for any of the product then replace it by ‚Äú-‚Äú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d85ec528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link Scaping From Pages -  1\n",
      "Link Scaping From Pages -  2\n",
      "Link Scaping From Pages -  3\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.amazon.in/')\n",
    "time.sleep(2)\n",
    "search_bar = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search_bar.send_keys('Guitar')\n",
    "search_button = driver.find_element(By.XPATH,'//span[@id=\"nav-search-submit-text\"]')\n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "product_link = []\n",
    "for page in range(0,3):\n",
    "    print(\"Link Scaping From Pages - \",page+1)\n",
    "    product_url = driver.find_elements(By.XPATH,'//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-4\"]/a')\n",
    "    for i in product_url:\n",
    "        product_link.append(i.get_attribute('href'))\n",
    "        try:\n",
    "            next_page = driver.find_element(By.XPATH,'//div[@class=\"a-section a-text-center s-pagination-container\"]/span')\n",
    "            \n",
    "        except StaleElementReferenceException:\n",
    "            print('Exception Raised:-')\n",
    "            next_page1 = driver.find_element(By.XPATH,'//div[@class=\"a-section a-text-center s-pagination-container\"]/span')\n",
    "            next_page1.click()\n",
    "Brand_Name = []\n",
    "Name_of_the_Product = []\n",
    "Price = []\n",
    "Return_Exchange = []\n",
    "Expected_Delivery = []\n",
    "Availability = []\n",
    "\n",
    "for i in product_link:\n",
    "    driver.get(i)\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element(By.XPATH,'//tr[@class=\"a-spacing-small po-brand\"]/td[@class=\"a-span9\"]/span')\n",
    "        Brand_Name.append(brand.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Brand_Name.append('-')\n",
    "    try:    \n",
    "        name = driver.find_element(By.XPATH,'//div[@id=\"titleSection\"]/h1/span')\n",
    "        Name_of_the_Product.append(name.text)\n",
    "        \n",
    "    except NoSuchElementException:       \n",
    "        Name_of_the_Product.append('-')\n",
    "            \n",
    "    try:    \n",
    "        price = driver.find_element(By.XPATH,'//span[@class=\"a-price aok-align-center reinventPricePriceToPayMargin priceToPay\"]//span[@class=\"a-price-whole\"]')\n",
    "        Price.append(price.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')\n",
    "       \n",
    "    try:     \n",
    "        exchange = driver.find_element(By.XPATH,'/html/body/div[2]/div/div[5]/div[3]/div[4]/div[24]/div[2]/div/div/div/div[2]/div/ol/li[3]/div/span/div/span')\n",
    "        Return_Exchange.append(exchange.text)\n",
    "        \n",
    "    except NoSuchElementException:       \n",
    "        Return_Exchange.append('-')\n",
    "       \n",
    "    try:    \n",
    "        delivery= driver.find_element(By.XPATH,'//div[@id=\"deliveryBlockMessage\"]/div/div/span')\n",
    "        Expected_Delivery.append(delivery.text)\n",
    "        \n",
    "    except NoSuchElementException:      \n",
    "        Expected_Delivery.append('-')\n",
    "         \n",
    "    try:\n",
    "        available= driver.find_element(By.XPATH,'//div[@id=\"availability\"]/span')\n",
    "        Availability.append(available.text)\n",
    "        \n",
    "    except NoSuchElementException:        \n",
    "        Availability.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c79a7491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 180 180 180 180 180\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand_Name),len(Name_of_the_Product),len(Price),len(Return_Exchange),len(Expected_Delivery),len(Availability))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "131660e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand_Name</th>\n",
       "      <th>Name_of_the_Product</th>\n",
       "      <th>Price</th>\n",
       "      <th>Return_Exchange</th>\n",
       "      <th>Expected_Delivery</th>\n",
       "      <th>Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAULT</td>\n",
       "      <td>Vault DA20 Dreadnought Acoustic Guitar With Gi...</td>\n",
       "      <td>4,899</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Tuesday, 26 December. Order with...</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAULT</td>\n",
       "      <td>Vault Traveller 34 Inch Acoustic Guitar With T...</td>\n",
       "      <td>4,299</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Tuesday, 26 December. Order with...</td>\n",
       "      <td>Only 1 left in stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Henrix</td>\n",
       "      <td>Henrix 38C 38 Inch Cutaway Acoustic Guitar Wit...</td>\n",
       "      <td>3,099</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Tuesday, 26 December. Details</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allmusicmart</td>\n",
       "      <td>‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"DOLPHIN\" Guitar Combo P...</td>\n",
       "      <td>3,550</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Monday, 25 December. Order withi...</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JUAREZ</td>\n",
       "      <td>Ju√¢rez Acoustic Guitar, 38 Inch Cutaway, 038C ...</td>\n",
       "      <td>1,999</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Monday, 25 December. Order withi...</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence GenZ Guitar (with Guitar learning cour...</td>\n",
       "      <td>5,999</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Sunday, 24 December. Details</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Kadence</td>\n",
       "      <td>Kadence Slowhand Premium Jumbo Semi Acoustic G...</td>\n",
       "      <td>8,999</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Friday, 29 December. Details</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>allmusicmart</td>\n",
       "      <td>‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"DOLPHIN\"Guitar Combo Pa...</td>\n",
       "      <td>3,550</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Monday, 25 December. Order withi...</td>\n",
       "      <td>Only 1 left in stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>allmusicmart</td>\n",
       "      <td>‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"DOLPHIN\" Guitar Combo P...</td>\n",
       "      <td>3,550</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Monday, 25 December. Order withi...</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>allmusicmart</td>\n",
       "      <td>‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"BOLT\" Guitar Combo Pack...</td>\n",
       "      <td>2,750</td>\n",
       "      <td>7 days Replacement</td>\n",
       "      <td>FREE delivery Monday, 25 December. Order withi...</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand_Name                                Name_of_the_Product  Price  \\\n",
       "0           VAULT  Vault DA20 Dreadnought Acoustic Guitar With Gi...  4,899   \n",
       "1           VAULT  Vault Traveller 34 Inch Acoustic Guitar With T...  4,299   \n",
       "2          Henrix  Henrix 38C 38 Inch Cutaway Acoustic Guitar Wit...  3,099   \n",
       "3    allmusicmart  ‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"DOLPHIN\" Guitar Combo P...  3,550   \n",
       "4          JUAREZ  Ju√¢rez Acoustic Guitar, 38 Inch Cutaway, 038C ...  1,999   \n",
       "..            ...                                                ...    ...   \n",
       "175       Kadence  Kadence GenZ Guitar (with Guitar learning cour...  5,999   \n",
       "176       Kadence  Kadence Slowhand Premium Jumbo Semi Acoustic G...  8,999   \n",
       "177  allmusicmart  ‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"DOLPHIN\"Guitar Combo Pa...  3,550   \n",
       "178  allmusicmart  ‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"DOLPHIN\" Guitar Combo P...  3,550   \n",
       "179  allmusicmart  ‚ÄúALL WOODEN‚Äù 38‚Äù INCH \"BOLT\" Guitar Combo Pack...  2,750   \n",
       "\n",
       "        Return_Exchange                                  Expected_Delivery  \\\n",
       "0    7 days Replacement  FREE delivery Tuesday, 26 December. Order with...   \n",
       "1    7 days Replacement  FREE delivery Tuesday, 26 December. Order with...   \n",
       "2    7 days Replacement        FREE delivery Tuesday, 26 December. Details   \n",
       "3    7 days Replacement  FREE delivery Monday, 25 December. Order withi...   \n",
       "4    7 days Replacement  FREE delivery Monday, 25 December. Order withi...   \n",
       "..                  ...                                                ...   \n",
       "175  7 days Replacement         FREE delivery Sunday, 24 December. Details   \n",
       "176  7 days Replacement         FREE delivery Friday, 29 December. Details   \n",
       "177  7 days Replacement  FREE delivery Monday, 25 December. Order withi...   \n",
       "178  7 days Replacement  FREE delivery Monday, 25 December. Order withi...   \n",
       "179  7 days Replacement  FREE delivery Monday, 25 December. Order withi...   \n",
       "\n",
       "              Availability  \n",
       "0                 In stock  \n",
       "1    Only 1 left in stock.  \n",
       "2                 In stock  \n",
       "3                 In stock  \n",
       "4                 In stock  \n",
       "..                     ...  \n",
       "175               In stock  \n",
       "176               In stock  \n",
       "177  Only 1 left in stock.  \n",
       "178               In stock  \n",
       "179               In stock  \n",
       "\n",
       "[180 rows x 6 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Amazon_Data = pd.DataFrame({\"Brand_Name\":Brand_Name,\"Name_of_the_Product\":Name_of_the_Product,\"Price\":Price,\"Return_Exchange\":Return_Exchange,\"Expected_Delivery\":Expected_Delivery,\"Availability\":Availability})\n",
    "Amazon_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8178",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‚Äòfruits‚Äô, ‚Äòcars‚Äô and ‚ÄòMachine Learning‚Äô, ‚ÄòGuitar‚Äô, ‚ÄòCakes‚Äô. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9640217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get('https://www.google.com/')   \n",
    "image_button = driver.find_element(By.XPATH, '//div[@class=\"gb_Ad gb_C gb_Wf gb_Nf\"]/div[2]/a')\n",
    "image_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Keywords\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "for keyword in keywords:\n",
    "    try:\n",
    "        search_thing = driver.find_element(By.XPATH, '//textarea[@jsname=\"yZiJbe\"]')\n",
    "        search_thing.click()\n",
    "        search_thing.send_keys(keyword)\n",
    "    except NoSuchElementException:\n",
    "        search_thing1 = driver.find_element(By.XPATH, '/html/body/c-wiz/c-wiz/div/div[3]/div[2]/div/div[1]/form/div[1]/div[2]/div/div[2]/input')\n",
    "        search_thing1.send_keys(keyword)\n",
    "\n",
    "    search_button = driver.find_element(By.XPATH, '//button[@jsname=\"Tg7LZd\"]')\n",
    "    search_button.click()\n",
    "    \n",
    "    # Wait for some time to load search results\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Find all images on the page using Selenium\n",
    "    images = driver.find_elements(By.XPATH, '//img')\n",
    "    \n",
    "    # Create a directory for the keyword\n",
    "    keyword_path = Path(keyword)\n",
    "    keyword_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Download the first 10 images for the current keyword\n",
    "    for i, img in enumerate(images[:10], start=1):\n",
    "        img_url = img.get_attribute('src')\n",
    "        img_data = requests.get(img_url).content\n",
    "        img_filename = f\"{keyword}_{i}.jpg\"\n",
    "        img_path = keyword_path / img_filename\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "        \n",
    "        # Display the image in the Jupyter Notebook\n",
    "        display(Image(filename=str(img_path)))\n",
    "    \n",
    "    # Clear the search bar for the next keyword\n",
    "    search_word = driver.find_element(By.XPATH, '/html/body/c-wiz/c-wiz/div/div[3]/div[2]/div/div[1]/form/div[1]/div[2]/div/div[2]/input')\n",
    "    search_word.clear()\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4ecdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a4302aa",
   "metadata": {},
   "source": [
    "### 4.Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: ‚ÄúBrand Name‚Äù, ‚ÄúSmartphone name‚Äù, ‚ÄúColour‚Äù, ‚ÄúRAM‚Äù, ‚ÄúStorage(ROM)‚Äù, ‚ÄúPrimary Camera‚Äù, ‚ÄúSecondary Camera‚Äù, ‚ÄúDisplay Size‚Äù, ‚ÄúBattery Capacity‚Äù, ‚ÄúPrice‚Äù, ‚ÄúProduct URL‚Äù. Incase if any of the details is missing then replace it by ‚Äú- ‚Äú. Save your results in a dataframe and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa489c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(2)\n",
    "close_signin_tab = driver.find_element(By.XPATH,'//div[@class=\"JFPqaw\"]/span').click()\n",
    "\n",
    "search_bar = driver.find_element(By.XPATH,'//div[@class=\"_2SmNnR\"]/input[@class=\"Pke_EE\"]')\n",
    "search_bar.send_keys('Oneplus Nord')\n",
    "\n",
    "search_button = driver.find_element(By.XPATH,'//div[@class=\"_1sFryS _2alaMB\"]/button').click()\n",
    "product_link = []\n",
    "product_url = driver.find_elements(By.XPATH,'//div[@class=\"_2kHMtA\"]/a')\n",
    "for i in product_url:\n",
    "    product_link.append(i.get_attribute('href'))\n",
    "time.sleep(2)\n",
    "Brand_Name = []\n",
    "Smartphone_Name = []\n",
    "Colour = []\n",
    "RAM = []\n",
    "Storage_ROM = []\n",
    "Primary_Camera = []\n",
    "Secondary_Camera = []\n",
    "Display_Size = []\n",
    "Battery_Capacity = []\n",
    "Price  = []\n",
    "for i in product_link:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        b_name = driver.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]')\n",
    "        Brand_Name.append(b_name.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Brand_Name.append('-')\n",
    "    try:\n",
    "        s_name = driver.find_element(By.XPATH,'//div[@class=\"_1AtVbE col-12-12\"]/div/div/div[@class=\"_1UhVsV _3AsE0T\"][1]/div/table/tbody/tr[@class=\"_1s_Smc row\"][3]/td[@class=\"URwL2w col col-9-12\"][1]')\n",
    "        Smartphone_Name.append(s_name.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Smartphone.append('-')\n",
    "    try:\n",
    "        clr = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[1]/table/tbody/tr[4]/td[2]/ul/li')\n",
    "        Colour.append(clr.text) \n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Colour.append('-')               \n",
    "    try:\n",
    "        rm = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[4]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        RAM.append(rm.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        RAM.append('-')\n",
    "    try:\n",
    "        strg = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Storage_ROM.append(strg.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Storage_ROM.append('-')\n",
    "        \n",
    "    try:\n",
    "        p_camera = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[5]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Primary_Camera.append(p_camera.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Primary_Camera.append('-')\n",
    "    try:\n",
    "        s_camera =driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Secondary_Camera.append(s_camera.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Secondary_Camera.append('-')\n",
    "    try:\n",
    "        display = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Display_Size.append(display.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Display_Size.append('-')\n",
    "        \n",
    "    try:\n",
    "        battery = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div[4]/div/div[2]/div[1]/div[7]/table/tbody/tr/td[2]/ul/li') \n",
    "        Battery_Capacity.append(battery.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Battery_Capacity.append('-')\n",
    "    \n",
    "    try:\n",
    "        price = driver.find_element(By.XPATH,'//div[@class=\"CEmiEU\"]/div[@class=\"_25b18c\"]/div[1]')\n",
    "        Price.append(price.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed609020",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brand_Name),len(Smartphone_Name),len(Colour),len(RAM),len(Storage_ROM),len(Primary_Camera),len(Secondary_Camera),len(Display_Size),len(Battery_Capacity),len(Price))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470950c7",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_coordinates(city):\n",
    "    driver = webdriver.Chrome()  # Set up the Chrome webdriver\n",
    "    driver.get(\"https://www.google.com/maps\")  # Open Google Maps\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Find the search box and enter the city\n",
    "    search_box = driver.find_element_by_xpath(\"//input[@id='searchboxinput']\")\n",
    "    search_box.send_keys(city)\n",
    "    search_box.submit()\n",
    "    time.sleep(5)  # Wait for the search results to load\n",
    "\n",
    "    # Get the URL which contains the coordinates\n",
    "    current_url = driver.current_url\n",
    "\n",
    "    # Extract latitude and longitude from the URL using regex\n",
    "    coordinates = re.findall(r'@(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)', current_url)\n",
    "\n",
    "    driver.quit()  # Close the browser window\n",
    "\n",
    "    return coordinates[0] if coordinates else None\n",
    "\n",
    "# Replace 'Your_City' with the city you want to search\n",
    "city_name = 'New York'\n",
    "coordinates = get_coordinates(city_name)\n",
    "\n",
    "if coordinates:\n",
    "    latitude, longitude = coordinates\n",
    "    print(f\"Geospatial coordinates for {city_name}: Latitude {latitude}, Longitude {longitude}\")\n",
    "else:\n",
    "    print(f\"Could not find coordinates for {city_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e37287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5eae163",
   "metadata": {},
   "source": [
    "## 6. Write a program to scrap all the available details of best gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.digit.in/')\n",
    "time.sleep(2)\n",
    "search_bar = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div/div/div[2]/form/input[1]')\n",
    "search_bar.send_keys('gaming laptop')\n",
    "\n",
    "search_button = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div/div/div[2]/form/button').click()\n",
    "laptop_url = []\n",
    "laptoplink = driver.find_elements(By.XPATH,'//h3[@class=\" text-clamp text-clamp-2\"]/a')\n",
    "for i in laptoplink:\n",
    "    laptop_url.append(i.get_attribute('href'))    \n",
    "#Create Empty List\n",
    "laptop_name = []\n",
    "operating_system = []\n",
    "processor = []\n",
    "storage = []\n",
    "display_size = []\n",
    "ram = []\n",
    "laptop_weight = []\n",
    "battery_backup = []\n",
    "release_date = []\n",
    "overall_rating = []\n",
    "description = []\n",
    "price = []\n",
    "#Scrap Data Using Loop  For All Columns\n",
    "for i in laptop_url:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        lap_name = driver.find_element(By.XPATH,'//h1[@class=\"floatleft tabletblockdisplay pr20 \"]')\n",
    "        laptop_name.append(lap_name.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        laptop_name.append('-')\n",
    "    \n",
    "    try:\n",
    "        os = driver.find_element(By.XPATH,'//div[@class=\"key_specifications \"]/div/ul[1]/li[1]/div[1]/p[2]')\n",
    "        operating_system.append(os.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        operating_system.append('-')\n",
    "        \n",
    "    try:\n",
    "        pros = driver.find_element(By.XPATH,'//div[@class=\"key_specifications \"]/div/ul[1]/li[2]/div[1]/p[2]')\n",
    "        processor.append(pros.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        processor.append('-')\n",
    "        \n",
    "    try:\n",
    "        strg = driver.find_element(By.XPATH,'//div[@class=\"key_specifications \"]/div/ul[1]/li[3]/div[1]/p[2]')\n",
    "        storage.append(strg.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        storage.append('-')\n",
    "        \n",
    "    try:\n",
    "        display = driver.find_element(By.XPATH,'//div[@class=\"key_specifications \"]/div/ul[1]/li[4]/div[1]/p[2]')\n",
    "        display_size.append(display.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        display_size.append('-')\n",
    "    try:\n",
    "        rm = driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[1]/div/div/div/div/div[2]/div[4]/div[3]/div[2]/table/tbody/tr[8]/td/table/tbody/tr[1]/td')\n",
    "        ram.append(rm.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        ram.append('-')\n",
    "    try:\n",
    "        weight = driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[1]/div/div/div/div/div[2]/div[4]/div[3]/div[2]/table/tbody/tr[10]/td/table/tbody/tr[2]/td')\n",
    "        laptop_weight.append(weight.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        laptop_weight.append('-')\n",
    "        \n",
    "    try:\n",
    "        battery = driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[1]/div/div/div/div/div[2]/div[4]/div[3]/div[2]/table/tbody/tr[20]/td/table/tbody/tr/td')\n",
    "        battery_backup.append(battery.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        battery_backup.append('-')\n",
    "        \n",
    "    try:\n",
    "        date = driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[1]/div/div/div/div/div[2]/div[1]/div[4]/div[1]/div[2]')\n",
    "        release_date.append(date.text)\n",
    "   \n",
    "    except NoSuchElementException:\n",
    "        release_date.append('-')\n",
    "        \n",
    "    try:\n",
    "        rating = driver.find_element(By.XPATH,'//div[@class=\"rating_sec\"]')\n",
    "        overall_rating.append(rating.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        overall_rating.append('-')\n",
    "    \n",
    "    try:\n",
    "        descp = driver.find_element(By.XPATH,'//div[@class=\"review-text\"]/div/p')\n",
    "        description.append(descp.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        description.append('-')\n",
    "        \n",
    "    try:\n",
    "        prc = driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[1]/div/div/div/div/div[2]/div[3]/div/div[3]/div[1]/p/span/bdi/span')\n",
    "        price.append(prc.text)\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        price.append('-')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1958d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23072ec8",
   "metadata": {},
   "source": [
    "### 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: ‚ÄúRank‚Äù, ‚ÄúName‚Äù, ‚ÄúNet worth‚Äù, ‚ÄúAge‚Äù, ‚ÄúCitizenship‚Äù, ‚ÄúSource‚Äù, ‚ÄúIndustry‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e10d8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('http://www.forbes.com/')\n",
    "driver.maximize_window()\n",
    "time.sleep(10)\n",
    "side_button = driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div[1]/div/div').click()\n",
    "time.sleep(1)\n",
    "slide_button = driver.find_element(By.XPATH,'//li[@class=\"TjJgrPSg cD45ib6e primary\"]/div[@role=\"button\"]').click()\n",
    "time.sleep(2)\n",
    "billionaires_list_tag = driver.find_element(By.XPATH,'//div[@class=\"_4fMm9\"]/div/div[2]/div/a').click()\n",
    "Top_Rank = []\n",
    "Name =  []\n",
    "Net_Worth = []\n",
    "Age = []\n",
    "Citizenship = []\n",
    "Source = []\n",
    "Industry = []\n",
    "\n",
    "while True:\n",
    "             \n",
    "    rank = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_row__L-0Km\"]/div[1]')\n",
    "    for i in rank:\n",
    "        Top_Rank.append(i.text)\n",
    "        \n",
    "    name = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_row__L-0Km\"]/div[2]')    \n",
    "    for i in name:\n",
    "        Name.append(i.text)\n",
    "        \n",
    "    worth = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_row__L-0Km\"]/div[3]')    \n",
    "    for i in worth:\n",
    "        Net_Worth.append(i.text)\n",
    "    age = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_row__L-0Km\"]/div[4]')    \n",
    "    for i in age:\n",
    "        Age.append(i.text)\n",
    "    country = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_row__L-0Km\"]/div[5]')\n",
    "    for i in country:\n",
    "        Citizenship.append(i.text)\n",
    "    company = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_row__L-0Km\"]/div[6]')\n",
    "    for i in company:\n",
    "        Source.append(i.text)\n",
    "    indus  = driver.find_elements(By.XPATH,'//div[@class=\"TableRow_row__L-0Km\"]/div[7]')    \n",
    "    for i in indus:\n",
    "        Industry.append(i.text)       \n",
    "    try:\n",
    "        next_button1 = driver.find_element(By.XPATH,'//button[2][@class=\"Pagination_paginationBtn__UIBE-\"]')\n",
    "        next_button1.click()\n",
    "\n",
    "    except:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "caf3957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2427 2427 2427 2427 2427 2427 2427\n"
     ]
    }
   ],
   "source": [
    "Billionaires_data = pd.DataFrame({\"Top_Rank\":Top_Rank,\"Name\":Name,\"Net_Worth\":Net_Worth,\"Age\":Age,\"Citizenship\":Citizenship,\"Source\":Source,\"Industry\":Industry})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d10e0016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Top_Rank                      Name Net_Worth Age    Citizenship  \\\n",
      "0         202  Vincent Bollor√© & family    $9.5 B  71         France   \n",
      "1         202              Jim Pattison    $9.5 B  94         Canada   \n",
      "2         204        Ernesto Bertarelli    $9.4 B  57    Switzerland   \n",
      "3         204                 Wang Xing    $9.4 B  44          China   \n",
      "4         206              Brian Chesky    $9.3 B  41  United States   \n",
      "...       ...                       ...       ...  ..            ...   \n",
      "2422     2540                   Yu Rong      $1 B  51          China   \n",
      "2423     2540    Richard Yuengling, Jr.      $1 B  80  United States   \n",
      "2424     2540             Zhang Gongyun      $1 B  60          China   \n",
      "2425     2540    Zhang Guiping & family      $1 B  71          China   \n",
      "2426     2540               Inigo Zobel      $1 B  66    Philippines   \n",
      "\n",
      "                            Source               Industry  \n",
      "0                      Investments  Finance & Investments  \n",
      "1                      Diversified            Diversified  \n",
      "2             Biotech, investments             Healthcare  \n",
      "3                    Food delivery             Technology  \n",
      "4                           Airbnb             Technology  \n",
      "...                            ...                    ...  \n",
      "2422                Health clinics             Healthcare  \n",
      "2423                          Beer        Food & Beverage  \n",
      "2424  Tyre manufacturing machinery          Manufacturing  \n",
      "2425                   Real estate            Real Estate  \n",
      "2426                   Diversified            Diversified  \n",
      "\n",
      "[2427 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Billionaires_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43af712",
   "metadata": {},
   "source": [
    "# 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7a84ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get('https://www.youtube.com/')\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "499631b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element(By.XPATH,'//div[@id=\"search-input\"]/input')\n",
    "search_bar.send_keys('Maher Zain - Rahmatun Lil‚ÄôAlameen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21178139",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element(By.XPATH,'//button[@id=\"search-icon-legacy\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1850c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_video = driver.find_element(By.XPATH,'//div[@id=\"title-wrapper\"]/h3').click()\n",
    "time.sleep(5)\n",
    "for _ in range(200):\n",
    "    driver.execute_script(\"window.scrollBy(2,000)\")\n",
    "# creating empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Likes = []\n",
    "# scrape comments\n",
    "cm = driver.find_elements(By.XPATH,'//div[@id=\"body\"]/div[2]/div[2]')\n",
    "for i in cm[:500]:\n",
    "    comments.append(i.text)\n",
    "    \n",
    "# scrape time when comment was posted\n",
    "tm = driver.find_elements(By.XPATH,\"//a[contains(text(),'ago')]\")\n",
    "for i in tm[:500]:\n",
    "    comment_time.append(i.text)\n",
    "    \n",
    "# scrape the comment likes\n",
    "like = driver.find_elements(By.XPATH,\"//span[@class='style-scope ytd-comment-action-buttons-renderer'][2]\")\n",
    "for i in like[:500]:\n",
    "    Likes.append(i.text)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a125cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Comments Time</th>\n",
       "      <th>Comments Like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Proud that I'm Muslim. Thanks Allah Almighty.</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greetings from Romania. üá∑üá¥ Praying for all Pal...</td>\n",
       "      <td>12 days ago (edited)</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am proud to be a muslim from üáßüá©\\nAssalamuala...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm a Christian but I love to heard this song ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ÿ≠ÿ®Ÿäÿ®Ÿä Ÿäÿß ŸÖÿ≠ŸÖÿØ Ÿäÿß ÿ±ÿ≠ŸÖÿ© ŸÑŸÑÿπÿßŸÑŸÖŸäŸÜ ŸÖÿ≠ŸÖÿØ  Ô∑∫</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Beautiful nasheed ..I am  a Muslim ..thank you...</td>\n",
       "      <td>3 weeks ago</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Masya allah SWT</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Love you mohommad Ô∑∫</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Love you maher zain. You're amazing brother</td>\n",
       "      <td>13 days ago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>My heart</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comments         Comments Time  \\\n",
       "0        Proud that I'm Muslim. Thanks Allah Almighty.           2 weeks ago   \n",
       "1    Greetings from Romania. üá∑üá¥ Praying for all Pal...  12 days ago (edited)   \n",
       "2    I am proud to be a muslim from üáßüá©\\nAssalamuala...           1 month ago   \n",
       "3    I'm a Christian but I love to heard this song ...           2 weeks ago   \n",
       "4              ÿ≠ÿ®Ÿäÿ®Ÿä Ÿäÿß ŸÖÿ≠ŸÖÿØ Ÿäÿß ÿ±ÿ≠ŸÖÿ© ŸÑŸÑÿπÿßŸÑŸÖŸäŸÜ ŸÖÿ≠ŸÖÿØ  Ô∑∫            2 weeks ago   \n",
       "..                                                 ...                   ...   \n",
       "495  Beautiful nasheed ..I am  a Muslim ..thank you...           3 weeks ago   \n",
       "496                                    Masya allah SWT           1 month ago   \n",
       "497                               Love you mohommad Ô∑∫              1 day ago   \n",
       "498       Love you maher zain. You're amazing brother            13 days ago   \n",
       "499                                          My heart             6 days ago   \n",
       "\n",
       "    Comments Like  \n",
       "0             127  \n",
       "1              43  \n",
       "2             340  \n",
       "3             278  \n",
       "4              92  \n",
       "..            ...  \n",
       "495             6  \n",
       "496             2  \n",
       "497                \n",
       "498             1  \n",
       "499             1  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_data = pd.DataFrame({\"Comments\":comments,\"Comments Time\":comment_time,\"Comments Like\":Likes})\n",
    "youtube_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a56db0",
   "metadata": {},
   "source": [
    "# 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in ‚ÄúLondon‚Äù location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39ccb8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the web page of mentioned url\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "# locating the location search bar\n",
    "search_bar = driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div/div/div/div[1]/div[1]/div/div[2]/input')\n",
    "# entering London in search bar\n",
    "search_bar.send_keys(\"London\")\n",
    "time.sleep(2)\n",
    "# select London\n",
    "London = driver.find_element(By.XPATH,\"/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div/div/div/div[1]/div[2]/div/ul/li[2]\").click()\n",
    "# do click on Let's Go button\n",
    "search_btn = driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[2]/div[2]/div/div/div/div[5]/button[2]')\n",
    "search_btn.click()\n",
    "time.sleep(3)\n",
    "#Scrap Data\n",
    "\n",
    "Distance_City_Center =[]  #Create Empty List\n",
    "distance_tag = driver.find_elements(By.XPATH,'//div[@class=\"property-distance\"]')\n",
    "for i in distance_tag:\n",
    "    Distance_City_Center.append(i.text)\n",
    "    \n",
    "Dorm_From_Price = []          #Create Empty List\n",
    "d_price_tag = driver.find_elements(By.XPATH,'//div[@class=\"property-accommodation-prices\"]/div[2]')\n",
    "for i in d_price_tag:\n",
    "    Dorm_From_Price.append(i.text)\n",
    "    \n",
    "Private_Price = []     #Create Empty List\n",
    "p_price_tag = driver.find_elements(By.XPATH,'//div[@class=\"property-accommodation-prices\"]/div[1]')\n",
    "for i in p_price_tag:\n",
    "    Private_Price.append(i.text)\n",
    "hostal_link = []     #Create Empty List\n",
    "hostal_url = driver.find_elements(By.XPATH,'//div[@class=\"property-card\"]/a') #XPATH of LINks\n",
    "for i in hostal_url:\n",
    "    hostal_link.append(i.get_attribute('href'))\n",
    "Hostal_Name = []      #Create Empty List\n",
    "Hostal_Description = []         #Create Empty List\n",
    "Facilities = []                #Create Empty List\n",
    "Over_All_Rating = []            #Create Empty List\n",
    "Total_Review = []            #Create Empty List\n",
    "for i in hostal_link:\n",
    "    driver.get(i)\n",
    "    \n",
    "    try:\n",
    "        property_name = driver.find_element(By.XPATH,'//div[@class=\"headline-container\"]/div/h1')\n",
    "        Hostal_Name.append(property_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Hostal_Name.append('-')\n",
    "    \n",
    "    try:\n",
    "        description_tag = driver.find_element(By.XPATH,'//div[@class=\"description-container\"]')\n",
    "        Hostal_Description.append(description_tag.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Hostal_Description.append('-')\n",
    "        \n",
    "    try:\n",
    "        f_tag = driver.find_element(By.XPATH,'//div[@class=\"facilities-container\"]/ul/li/ul')\n",
    "        Facilities.append(f_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        Facilities.append('-')\n",
    "    \n",
    "    try:\n",
    "        rating_tag = driver.find_element(By.XPATH,'//div[@class=\"rating rating-summary-container big clickable\"][1]/div[1]')\n",
    "        Over_All_Rating.append(rating_tag.text)\n",
    "    except NoSuchElementException:\n",
    "        Over_All_Rating.append('-')\n",
    "    \n",
    "    try:\n",
    "        t_review = driver.find_element(By.XPATH,'//div[@class=\"rating rating-summary-container big clickable\"][1]/div[2]/div[2]')\n",
    "        Total_Review.append(t_review.text)\n",
    "    except NoSuchElementException:\n",
    "        Total_Review.append('-')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2bfea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d982e443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 33 33 33 30 33 33 30\n"
     ]
    }
   ],
   "source": [
    "print(len(Distance_City_Center),len(Hostal_Name),len(Hostal_Description),len(Facilities),len(Dorm_From_Price),len(Over_All_Rating),len(Total_Review),len(Private_Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f0d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef805080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec827247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbbbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
